{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_1_Machine_Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nazmul-karim170/Homework-1-ML/blob/master/Homework_1_Machine_Learning_course.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "u5RXinm7wv4y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**First, we import the necessary python packages that we need for our homework.**"
      ]
    },
    {
      "metadata": {
        "id": "6HKA2JZ9u_Y6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m2rDnkhaxeio",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Then we load the data and normalize them**"
      ]
    },
    {
      "metadata": {
        "id": "XSSgDFfpxdf8",
        "colab_type": "code",
        "outputId": "02786c30-a3de-4bf2-a752-6d44f5342cd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], 28*28)/255\n",
        "x_test  = x_test.reshape(x_test.shape[0], 28*28)/255"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V6EK3yt3z1CV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Here are some activation and loss functions we are going to need for solving our problems. I am putting it all here together.  **"
      ]
    },
    {
      "metadata": {
        "id": "cr0bAZAw0d-c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1/(1+ np.exp(-z))\n",
        "\n",
        "  \n",
        "## differentiation of sigmoid \n",
        "def sigmoid_dif(x):\n",
        "    return sigmoid(x)*(1-sigmoid(x))\n",
        "\n",
        "## softmax function  \n",
        "def softmax(z):\n",
        "    z -= np.max(z)\n",
        "    sm = (np.exp(z).T/ np.sum(np.exp(z), axis=1)).T\n",
        "    return sm  \n",
        "\n",
        "  \n",
        "#### mean square error loss   \n",
        "def mse_loss(y, y_hat):\n",
        "    return 0.5*np.mean((y_hat-y)**2)\n",
        "  \n",
        "## binary cross entropy loss \n",
        "def logistic_loss(y, y_hat):\n",
        "    return -np.mean(y* np.log(y_hat) + (1-y) * np.log(1-y_hat))  \n",
        "\n",
        "  \n",
        "## categorical cross entropy loss  \n",
        "def CCE_loss(y, y_hat):\n",
        "    return -np.mean(y * np.log(y_hat)) \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6BKOBIBC2Our",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**We define the minibatch size and number of epochs here for our training.**"
      ]
    },
    {
      "metadata": {
        "id": "Rmp793eI2jmA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "minibatch_size = 40\n",
        "n_epochs   =  20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iwkPmmEf3GHk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Problem 1: Designing ten classifier using logistic regression with mean square error loss function** Here, we have ten different classifier for each class. The parameters we have, after training, are different for each class."
      ]
    },
    {
      "metadata": {
        "id": "63ac8_H93V_Y",
        "colab_type": "code",
        "outputId": "a8af40b7-e2f4-4ead-eb81-e79b7b78449b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "##Forward propagation caculation\n",
        "def forward(x,W,b):\n",
        "    Z = np.dot(x,W) + b\n",
        "    A = sigmoid(Z)\n",
        "    return A\n",
        "\n",
        "##Learning rate\n",
        "lr = 0.05 \n",
        "\n",
        "##Define the number of features and classes\n",
        "n_feature = 28*28               #input size(could be whole image or just the extracted features)\n",
        "n_class = 1                     #output size\n",
        "\n",
        "##Accumulated_Loss\n",
        "loss_a =[]   \n",
        "\n",
        "##stacking up all the parameters of all classifiers\n",
        "model_parameters_w = []\n",
        "model_parameters_b = []\n",
        "\n",
        "## We do it for ten classes that's why we are using the for loop\n",
        "\n",
        "for label in range(10):  \n",
        "    \n",
        "    ## Weight_initialization\n",
        "    W = np.random.randn(n_feature)\n",
        "    b = np.random.randn(n_class) \n",
        "    \n",
        "    ### TRAINING PHASE\n",
        "    for epoch in range(n_epochs):\n",
        "               \n",
        "        ##Shuffling the dataset for each epoch\n",
        "        m= np.random.permutation(x_train.shape[0])\n",
        "        x_train = x_train[m]\n",
        "        y_train = y_train[m]\n",
        "        \n",
        "        for i in range(int(x_train.shape[0]/ minibatch_size)):\n",
        "            \n",
        "            #minibatch_training\n",
        "            x_train_m = x_train[i:i+minibatch_size]\n",
        "            y_train_m = y_train[i:i+minibatch_size]\n",
        "            \n",
        "            ##get the value\n",
        "            A = forward(x_train_m, W, b) \n",
        "\n",
        "            ## Create probability distribution of true label        \n",
        "            y = y_train_m==label\n",
        "\n",
        "            ## Mean Square loss\n",
        "            loss = mse_loss(y, A)\n",
        "\n",
        "            ## gradient calculation\n",
        "            dz = sigmoid_dif(A)*(A - y)/minibatch_size\n",
        "\n",
        "\n",
        "            ## Update of W and b\n",
        "            dw = np.matmul(x_train_m.T,dz)\n",
        "            db = np.sum(dz, axis = 0)\n",
        "\n",
        "            W = W - lr*dw\n",
        "            b = b - lr*db\n",
        "            loss_a.append(loss)\n",
        "       \n",
        "    \n",
        "    ###TESTING PHASE    \n",
        "    y_p = forward(x_test, W,b)>=0.5\n",
        "    y_test1 = y_test == label\n",
        "    accs = np.sum(y_p == y_test1)/y_test.shape[0] \n",
        "\n",
        "    print('The accuracy of the classifier designed for class label {} is:: {}'.format(label,accs))\n",
        "    \n",
        "    model_parameters_w.append(W) \n",
        "    model_parameters_b.append(b)\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of the classifier designed for class label 0 is:: 0.9863\n",
            "The accuracy of the classifier designed for class label 1 is:: 0.9882\n",
            "The accuracy of the classifier designed for class label 2 is:: 0.9637\n",
            "The accuracy of the classifier designed for class label 3 is:: 0.9627\n",
            "The accuracy of the classifier designed for class label 4 is:: 0.9727\n",
            "The accuracy of the classifier designed for class label 5 is:: 0.9598\n",
            "The accuracy of the classifier designed for class label 6 is:: 0.9794\n",
            "The accuracy of the classifier designed for class label 7 is:: 0.977\n",
            "The accuracy of the classifier designed for class label 8 is:: 0.9453\n",
            "The accuracy of the classifier designed for class label 9 is:: 0.9527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "guGCCGbd87Ft",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we take the parameters of all the classifiers and evaluate the accuracy by using argmax operation."
      ]
    },
    {
      "metadata": {
        "id": "962pmOP286Hd",
        "colab_type": "code",
        "outputId": "4d3b6d65-86d4-4db4-b15a-ddcee885060a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "## classification accuracy if we make a joint classifier for all classes     \n",
        "Y_P = np.zeros((x_test.shape[0], 10))\n",
        "for label in range(10):\n",
        "  Y_P[:,label] = np.squeeze(forward(x_test,model_parameters_w[label], model_parameters_b[label]))\n",
        "   \n",
        "index = np.argmax(Y_P, axis = 1) \n",
        "acc = np.sum(index == y_test)/y_test.shape[0]\n",
        "\n",
        "print (\"The overall accuracy is now: %f\" %acc)    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The overall accuracy is now: 0.859200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wHOhHb7BDoO5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Problem 2: Designing ten classifier using logistic regression binary cross entropy loss function** Here, we do the same thing as problem 1 just use different loss function. the code are almost same."
      ]
    },
    {
      "metadata": {
        "id": "cXKAQ86_DxWH",
        "colab_type": "code",
        "outputId": "1606c21c-900a-4ae0-fadc-e91822d529a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "##Forward propagation caculation\n",
        "def forward(x,W,b):\n",
        "    Z = np.dot(x,W) + b\n",
        "    A = sigmoid(Z)\n",
        "    return A\n",
        "\n",
        "##Learning rate\n",
        "lr = 0.005 \n",
        "\n",
        "##Define the number of features and classes\n",
        "n_feature = 28*28               #input size(could be whole image or just the extracted features)\n",
        "n_class = 1                     #output size\n",
        "\n",
        "##Accumulated_Loss\n",
        "loss_a =[]   \n",
        "\n",
        "##stacking up all the parameters of all classifiers\n",
        "model_parameters_w = []\n",
        "model_parameters_b = []\n",
        "\n",
        "## We do it for ten classes that's why we are using the for loop\n",
        "\n",
        "for label in range(10):  \n",
        "    \n",
        "    ## Weight_initialization\n",
        "    W = np.random.randn(n_feature)\n",
        "    b = np.random.randn(n_class) \n",
        "    \n",
        "    ### TRAINING PHASE\n",
        "    for epoch in range(n_epochs):\n",
        "               \n",
        "        ##Shuffling the dataset for each epoch\n",
        "        m= np.random.permutation(x_train.shape[0])\n",
        "        x_train = x_train[m]\n",
        "        y_train = y_train[m]\n",
        "        \n",
        "        for i in range(int(x_train.shape[0]/ minibatch_size)):\n",
        "            \n",
        "            #minibatch_training\n",
        "            x_train_m = x_train[i:i+minibatch_size]\n",
        "            y_train_m = y_train[i:i+minibatch_size]\n",
        "            \n",
        "            ##get the value\n",
        "            A = forward(x_train_m, W, b) \n",
        "\n",
        "            ## Create probability distribution of true label        \n",
        "            y = y_train_m==label\n",
        "\n",
        "            ## Binary Cross Entropy loss\n",
        "            loss = logistic_loss(y, A)\n",
        "\n",
        "            ## gradient calculation\n",
        "            dz = (A - y)/minibatch_size\n",
        "\n",
        "            ## Update of W and b\n",
        "            dw = np.matmul(x_train_m.T,dz)\n",
        "            db = np.sum(dz, axis = 0)\n",
        "\n",
        "            W = W - lr*dw\n",
        "            b = b - lr*db\n",
        "            loss_a.append(loss)\n",
        "       \n",
        "    \n",
        "    ###TESTING PHASE    \n",
        "    y_p = forward(x_test, W,b)>=0.5\n",
        "    y_test1 = y_test == label\n",
        "    accs = np.sum(y_p == y_test1)/y_test.shape[0] \n",
        "\n",
        "    print('The accuracy of the classifier designed for class label {} is:: {}'.format(label,accs))\n",
        "    \n",
        "    model_parameters_w.append(W) \n",
        "    model_parameters_b.append(b)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of the classifier designed for class label 0 is:: 0.9828\n",
            "The accuracy of the classifier designed for class label 1 is:: 0.9857\n",
            "The accuracy of the classifier designed for class label 2 is:: 0.9652\n",
            "The accuracy of the classifier designed for class label 3 is:: 0.9597\n",
            "The accuracy of the classifier designed for class label 4 is:: 0.9635\n",
            "The accuracy of the classifier designed for class label 5 is:: 0.9542\n",
            "The accuracy of the classifier designed for class label 6 is:: 0.9764\n",
            "The accuracy of the classifier designed for class label 7 is:: 0.9737\n",
            "The accuracy of the classifier designed for class label 8 is:: 0.9328\n",
            "The accuracy of the classifier designed for class label 9 is:: 0.9461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v1VC4cuFHJeI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we take the parameters of all the classifiers and evaluate the accuracy by using argmax operation."
      ]
    },
    {
      "metadata": {
        "id": "I-gZdw82HJ6_",
        "colab_type": "code",
        "outputId": "b7e28de0-f19b-4d54-a723-c1bbaa7de861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "## classification accuracy if we make a joint classifier for all classes     \n",
        "Y_P = np.zeros((x_test.shape[0], 10))\n",
        "for label in range(10):\n",
        "  Y_P[:,label] = np.squeeze(forward(x_test,model_parameters_w[label], model_parameters_b[label]))\n",
        "   \n",
        "index = np.argmax(Y_P, axis = 1) \n",
        "acc = np.sum(index == y_test)/y_test.shape[0]\n",
        "\n",
        "print (\"The overall accuracy is now: %f\" %acc)  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The overall accuracy is now: 0.859200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hybs4-4EI07g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Problelm 3: A classifier using softmax regression **"
      ]
    },
    {
      "metadata": {
        "id": "1W12e8qxJSPL",
        "colab_type": "code",
        "outputId": "06247f4b-516c-4288-bcb7-c764ec7f2794",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "##Learning rate \n",
        "lr = 0.05  \n",
        "\n",
        "##Define the number of features and classes\n",
        "n_feature = 28*28                #input size(could be whole image or just the extracted features)\n",
        "n_class = 10                     # output size\n",
        "    \n",
        "\n",
        "# Weight_initialization\n",
        "W = np.random.randn(n_feature, n_class)\n",
        "b = np.random.randn(1,n_class)\n",
        "\n",
        "\n",
        "## Create probability distribution of true label \n",
        "\n",
        "##One hot encoding\n",
        "y_train1 = np.zeros((60000,n_class))\n",
        "y_train = np.array(y_train)\n",
        "y_train1[np.arange(60000), y_train] = 1\n",
        "    \n",
        "  \n",
        "### TRAINING PHASE       \n",
        "for epoch in range(n_epochs):\n",
        "            \n",
        "    ##Shuffling\n",
        "    m= np.random.permutation(x_train.shape[0])\n",
        "    x_train = x_train[m]\n",
        "    y_train1 = y_train1[m]\n",
        "    \n",
        "    for i in range(int(x_train.shape[0]/ minibatch_size)):\n",
        "        \n",
        "        #minibatch_training\n",
        "        x_train_m = x_train[i:i+minibatch_size]\n",
        "        y_train_m = y_train1[i:i+minibatch_size]\n",
        "        \n",
        "        \n",
        "        A = forward(x_train_m, W, b) \n",
        "   \n",
        "        \n",
        "        ## Categorical Cross Entropy loss\n",
        "        loss = CCE_loss(y_train_m, A)\n",
        "        \n",
        "        ## gradient calculation\n",
        "        dz = (A - y_train_m)/minibatch_size  #if we vectorize the whole thing our gradient function look like this \n",
        "        \n",
        "        \n",
        "        ## Update of W and b\n",
        "        dw = np.matmul(x_train_m.T,dz)\n",
        "        db = np.sum(dz, axis = 0)\n",
        "        \n",
        "        W = W - lr*dw\n",
        "        b = b - lr*db\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "###TESTING PHASE        \n",
        "y_p = np.argmax(forward(x_test, W,b),axis = 1)\n",
        "accs = np.sum(y_p == y_test)/y_test.shape[0] \n",
        "\n",
        "print(\"The accuracy for MNIST dataset using softmax is:\", accs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy for MNIST dataset using softmax is: 0.8869\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ybq1DwCqK-ml",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Problem 4: Implemeting problem 3 with keras**"
      ]
    },
    {
      "metadata": {
        "id": "u1b2ZrO-LGMU",
        "colab_type": "code",
        "outputId": "343303db-6678-480e-b1d4-92a4429aba3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "##Load the data\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "# Normalize the input\n",
        "x_train = x_train.reshape(x_train.shape[0], 28*28)/255\n",
        "x_test  = x_test.reshape(x_test.shape[0], 28*28)/255\n",
        "\n",
        "n_feature = 28*28      # input size (could be whole image or just the extracted features)\n",
        "n_class = 10           # output size\n",
        "\n",
        "\n",
        "#One hot encoding\n",
        "y_train1 = np.zeros((60000,n_class))\n",
        "y_train = np.array(y_train)\n",
        "y_train1[np.arange(60000), y_train] = 1\n",
        "\n",
        "y_test1 = np.zeros((10000,n_class))\n",
        "y_test = np.array(y_test)\n",
        "y_test1[np.arange(10000), y_test] = 1\n",
        "\n",
        "#Neural Network input size\n",
        "input_size = x_train.shape[1]\n",
        "\n",
        "\n",
        "\n",
        "# Using a dense layer with input and output size matched \n",
        "model = Sequential()\n",
        "model.add(Dense(output_dim = n_class, activation = 'softmax', input_dim = input_size))\n",
        "\n",
        "## Training and testing phase\n",
        "model.compile(loss='categorical_crossentropy', optimizer = 'sgd', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train1, epochs=10, batch_size = 40)\n",
        "model.evaluate(x_test, y_test1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", input_dim=784, units=10)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.8374 - acc: 0.8062\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.4821 - acc: 0.8761\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.4225 - acc: 0.8870\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.3927 - acc: 0.8932\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.3740 - acc: 0.8973\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.3607 - acc: 0.9001\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.3507 - acc: 0.9027\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.3428 - acc: 0.9044\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.3363 - acc: 0.9067\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.3309 - acc: 0.9078\n",
            "10000/10000 [==============================] - 0s 27us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3145777280688286, 0.9141]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "NTShiektN857",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Problem5 : Using DFS for new feature(It takes hour to run)**"
      ]
    },
    {
      "metadata": {
        "id": "yyhXtJYEdNeV",
        "colab_type": "code",
        "outputId": "2b1f90fa-9276-41b7-b172-11599cfc98bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.datasets import mnist\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train1 = np.round(x_train/255)\n",
        "x_test1 = np.round(x_test/255)\n",
        "\n",
        "def is_white(image):\n",
        "    a = image == 0\n",
        "    return a\n",
        "\n",
        "def get_adjacent(n):\n",
        "    x,y = n\n",
        "    return [(x-1,y),(x,y-1),(x+1,y),(x,y+1)]\n",
        "\n",
        "\n",
        "## DFS algorithm for for finding the number of connected components\n",
        "def DFS(start, end, pixels):\n",
        "    stack   = [start]\n",
        "    visited = []\n",
        "    ls = np.ndindex((28,28))\n",
        "    count =0                            \n",
        "    k = 0\n",
        "    s = start\n",
        "    while s != end:\n",
        "        if not is_white(pixels[s]):           ##if it is not white, append it to the visited list\n",
        "            visited.append(s)\n",
        "        if s not in visited:                  ## start from a white pixel  \n",
        "            stack.append(s)\n",
        "            while len(stack) != 0:            ## length of the stack should be empty at the end of traversing a connected region\n",
        "                x = stack[-1]\n",
        "                if is_white(pixels[x]):\n",
        "                    pixels[x] = 1             ## if it is white and you visited it, make it black pixel\n",
        "                    visited.append(x)\n",
        "\n",
        "                remove_from_stack = True\n",
        "                for adjacent in get_adjacent(x):\n",
        "                    if  adjacent in np.ndindex((28,28)):  ## adjacent pixel should be inside the image\n",
        "                        if is_white(pixels[adjacent]):  \n",
        "                            pixels[adjacent] = 1    \n",
        "                            stack.append(adjacent)\n",
        "                            visited.append(adjacent)\n",
        "                            remove_from_stack = False\n",
        "                if remove_from_stack:                     ## returning to the parent node\n",
        "                    stack.pop() \n",
        "            count += 1\n",
        "        k += 1  \n",
        "        y = next(ls)\n",
        "        if  y != end:\n",
        "            s = y\n",
        "        else:\n",
        "            s=start\n",
        "            \n",
        "        if k == pixels.shape[0]* pixels.shape[1]:\n",
        "            s = end\n",
        "    \n",
        "    return count\n",
        "\n",
        "\n",
        "## storing up the information from DFS\n",
        "count_train = np.zeros((x_train.shape[0],1))\n",
        "count_test  = np.zeros((x_test.shape[0],1))\n",
        "\n",
        "##RUN DFS for training dataset\n",
        "for i in range(x_train.shape[0]):\n",
        "    count_train[i,0] = DFS((27,27), (0,0), np.reshape(x_train1[i], [28,28]))\n",
        "\n",
        "\n",
        "x_train_dfs = np.append(np.reshape(x_train, 28*28)/255, count_train/3,axis =1)\n",
        "    \n",
        "## RUN DFS for test dataset\n",
        "for i in range(x_test.shape[0]):\n",
        "    count_test[i,0] = DFS((27,27), (0,0), np.reshape(x_test1[i], [28,28]) )\n",
        "    \n",
        "    \n",
        "x_test_dfs= np.append(np.reshape(x_test, 28*28)/255, count_test/3, axis =1)\n",
        "\n",
        "\n",
        "\n",
        "#One hot encoding\n",
        "y_train1 = np.zeros((60000,n_class))\n",
        "y_train = np.array(y_train)\n",
        "y_train1[np.arange(60000), y_train] = 1\n",
        "\n",
        "y_test1 = np.zeros((10000,n_class))\n",
        "y_test = np.array(y_test)\n",
        "y_test1[np.arange(10000), y_test] = 1\n",
        "\n",
        "#Neural Network input size\n",
        "input_size = x_train.shape[1]\n",
        "\n",
        "\n",
        "#Neural Network input size\n",
        "input_size = x_train_dfs.shape[1]\n",
        "\n",
        "#output size\n",
        "n_class = 10   \n",
        "\n",
        "\n",
        "# Using a dense layer with input and output size matched \n",
        "model = Sequential()\n",
        "model.add(Dense(output_dim = n_class, activation = 'softmax', input_dim = input_size))\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer = 'sgd', metrics=['accuracy'])\n",
        "model.fit(x_train_dfs, y_train1, epochs=100, batch_size = 40)\n",
        "model.evaluate(x_test_dfs, y_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:83: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", input_dim=785, units=10)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.8372 - acc: 0.8053\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.4756 - acc: 0.8780\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.4141 - acc: 0.8901\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.3831 - acc: 0.8970\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.3634 - acc: 0.9012\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.3492 - acc: 0.9046\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.3383 - acc: 0.9072\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.3295 - acc: 0.9094\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.3222 - acc: 0.9120\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.3161 - acc: 0.9130\n",
            "10000/10000 [==============================] - 0s 28us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2994290756046772, 0.9191]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    }
  ]
}